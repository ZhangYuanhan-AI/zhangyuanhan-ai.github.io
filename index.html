<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Yuanhan Zhang</title>
  
  <meta name="author" content="Yuanhan (John) Zhang">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
	<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üåê</text></svg>">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
          <tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p style="text-align:center">
                  <name>Yuanhan (John) Zhang</name> 
                </p>
                <p>Hi! I'm Yuanhan Zhang, here is the standard Chinese pronunciation for my first name : <a href="https://translate.google.com/?hl=zh-CN&sl=auto&tl=zh-CN&text=%E5%85%83%E7%80%9A&op=translate">Yuanhan</a>, a third-year PhD student at <a href="https://www.mmlab-ntu.com/">MMLab@NTU</a>, supervised by Prof. <a href="https://liuziwei7.github.io/">Ziwei Liu</a>.
                <br>

                <br>
                My research interests lie in computer vision and deep learning. In particular, I am focused on adapting foundation models‚Äîfrom vision to multi-modal‚Äîfor real-world exploration. This involves benchmarking model performance and adapting models through parameter-efficient tuning, in-context learning, instruction tuning.
                </p>
                <p style="text-align:center">
                  <a href="mailto:yuanhan002@e.ntu.edu.sg">Email</a> (yuanhan002@e.ntu.edu.sg) &nbsp/&nbsp
                  <a href="https://scholar.google.com/citations?user=g6grFy0AAAAJ&hl">Google Scholar</a> &nbsp/&nbsp
                  <a href="https://twitter.com/zhang_yuanhan">Twitter</a> &nbsp/&nbsp
                  <a href="https://github.com/ZhangYuanhan-AI">Github</a>
                </p>
              </td>
              <td style="padding:2.5%;width:40%;max-width:40%">
                <a href="images/zyh.jpeg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/zyh_circle2.png" class="hoverZoomLink"></a>
              </td>
            </tr>
          </tbody>
        </table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>News</heading>
            </td>
          </tr>
        </tbody></table>

        <ul>
          <li>
            [2025-7] We release Video Thinking Test (üìΩÔ∏è<a href="https://zhangyuanhan-ai.github.io/video-tt/">Video-TT </a>üìΩÔ∏è), a holistic benchmark to assess the advanced reasoning and understanding correctness/robustness between LLMs and humans.
          </li>
          <details>
            <summary>Older News & Activities</summary>
            <ul>
              <li>
                [2024-10] We update <a href="https://llava-vl.github.io/blog/2024-09-30-llava-video/">LLaVA-Video </a> (formerly LLaVA-NeXT-Video), releasing both the <a href="https://huggingface.co/collections/lmms-lab/llava-video-661e86f5e8dabc3ff793c944">model </a> and the <a href="https://huggingface.co/datasets/lmms-lab/LLaVA-Video-178K">Data </a>.
              </li>
              <li>
                [2024-08] We release <a href="https://llava-vl.github.io/blog/2024-08-05-llava-onevision/">LLaVA-OneVision</a>! A LMM that excels across single-image, multi-image, and video tasks.
              </li>
              <li>
                [2024-07] IJCV Outstanding Reviewer Award 2023.
              </li>
              <li>
                [2024-07] Finally, <a href="https://zhangyuanhan-ai.github.io/NOAH">NOAH</a> has been accepted by TPAMI.
              </li>
              <li>
                [2024-07] Three papers are accepted at ECCV 2024.
              </li>
              <li>
                [2024-06] We're organizing a workshop on <a href="https://prompting-in-vision.github.io/index_cvpr24.html">Prompting in Vision</a> at CVPR 2024.
              </li>
              <li>
                [2024-05] <a href="https://github.com/LLaVA-VL/LLaVA-NeXT">LLaVA-NeXT-Video</a> is released. Our team continues to build the most powerful open-source large modality model!
              </li>
              <li>
                [2023-09] <a href="https://github.com/ZhangYuanhan-AI/visual_prompt_retrieval">Visual Prompt Retrieval</a> is accepted in NeurIPS2023, see you in New Orleans!
              </li>
              <li>
                [2023-09] Gave a talk at Alibaba, Damo Academy, Hosted by Dr. <a href="https://lidongbing.github.io/">Lidong Bin.</a>
              </li>
              <li>
                [2023-07] Gave a talk at HITSZ, Hosted by Prof. <a href="https://rshaojimmy.github.io/">Rui Shao.</a>
              </li>
              <li>
                [2023-06] Introducing <a href="https://otter-ntu.github.io/">Otter</a>. Check it out now!
              </li>
              <li>
                [2022-10] We won the first place in <a href="https://computer-vision-in-the-wild.github.io/eccv-2022/">Computer Vision in the Wild Challenge</a>.
              </li>
              <li>
                [2022-07] <a href="https://zhangyuanhan-ai.github.io/OmniBenchmark">OmniBenchmark</a> is accepted in ECCV2022.
              </li>
              <li>
                [2022-03] <a href="https://github.com/Davidzhangyuanhan/Bamboo">Bamboo</a> dataset is released.
            </ul>
          </details>
          <!-- </li> -->
        </ul>


        <!-- <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading>Pre-prints</heading>
          </td>
        </tr>
      </tbody></table> -->




        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Publications</heading>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

          <tr>
            <td style="padding:5px;width:45%;vertical-align:middle">
              <div class="one">
                <img src='images/video-tt.png' width="360" height="130">
              </div>
            </td>
            <td style="padding:20px;width:55%;vertical-align:middle">
              <a href="https://zhangyuanhan-ai.github.io/video-tt/">
                <papertitle>Towards Video Thinking Test: A Holistic Benchmark for Advanced Video
                  Reasoning and Understanding</papertitle>
              </a>
              <br>
              <strong>Yuanhan Zhang<sup>*</sup></strong>,
              Yunice Chew<sup>*</sup>,
              <a href="https://scholar.google.com/citations?user=kMui170AAAAJ&hl=zh-CN">Yuhao Dong</a>,
              Aria Leo,
              Bo Hu,
              <a href='https://liuziwei7.github.io/' target='_blank'>Ziwei Liu</a>
              <!-- <br>
              *equal contribution,<span>&#9824;</span>project lead -->
              <br>
              <em>ICCV</em>, 2025
              <br>
              <a href='https://arxiv.org/abs/2507.15028' target='_blank'>PDF</a> / 
              <a href='https://huggingface.co/datasets/lmms-lab/video-tt' target='_blank'>Dataset and Code</a> 
              <p>  A holistic benchmark to assess the advanced reasoning and understanding correctness/robustness between LLMs and humans. </p>
            </td>
          </tr> 

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

            <tr>
              <td style="padding:5px;width:45%;vertical-align:middle">
                <div class="one">
                  <img src='images/llava-video.jpeg' width="300" height="180">
                </div>
              </td>
              <td style="padding:20px;width:55%;vertical-align:middle">
                <a href="https://llava-vl.github.io/blog/2024-09-30-llava-video">
                  <papertitle>LLaVA-Video: Video Instruction Tuning With Synthetic Data</papertitle>
                </a>
                <br>
                <strong>Yuanhan Zhang</strong>,
                <a href='https://scholar.google.com/citations?user=eh-XJIoAAAAJ&hl=zh-CN' target='_blank'>Jinming Wu</a>,
                <a href='https://scholar.google.com/citations?user=q8ZrKVIAAAAJ&hl=zh-CN' target='_blank'>Wei Li</a>,
                <a href='https://brianboli.com/' target='_blank'>Bo Li</a>,
                <a href='https://scholar.google.com/citations?user=XwY9LXoAAAAJ&hl=zh-CN' target='_blank'>Zejun Ma</a>,
                <a href='https://liuziwei7.github.io/' target='_blank'>Ziwei Liu</a>
                <a href='https://chunyuan.li/' target='_blank'>Chunyuan Li</a>
                <br>
                <em>TMLR</em>, 2025
                <br>
                <a href='https://arxiv.org/abs/2410.02713' target='_blank'>PDF</a> / 
                <a href='https://llava-vl.github.io/blog/2024-09-30-llava-video/' target='_blank'>Dataset, Model and Code</a> 
                <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/LLaVA-VL/LLaVA-NeXT?style=social"> 
                <p> Fully open-sourced video LMM model with competitive ability, including code, model, and data. </p>
              </td>
            </tr> 

          <tr>
            <td style="padding:5px;width:45%;vertical-align:middle">
              <div class="one">
                <img src='images/llava-onevision.png' width="340" height="150">
              </div>
            </td>
            <td style="padding:20px;width:55%;vertical-align:middle">
              <a href="https://llava-vl.github.io/blog/2024-08-05-llava-onevision/">
                <papertitle>LLaVA-OneVision: Easy Visual Task Transfer</papertitle>
              </a>
              <br>
              <a href='https://brianboli.com/' target='_blank'>Bo Li</a>,
              <strong>Yuanhan Zhang</strong>,
              <a href="https://www.linkedin.com/in/dongguoset">Dong Guo</a>,
              <a href="https://zrrskywalker.github.io/">Renrui Zhang</a>,
              <a href="https://fengli-ust.github.io/">Feng Li</a>,
              <a href="https://haozhang534.github.io/">Hao Zhang</a>,
              <a href="https://www.linkedin.com/in/kaichen-zhang-014b17219/?originalSubdomain=sg">Kaichen Zhang</a>,
              <a href="https://yanwei-li.com/">Yanwei Li</a>,
              <a href='https://liuziwei7.github.io/' target='_blank'>Ziwei Liu</a>
              <a href='https://chunyuan.li/' target='_blank'>Chunyuan Li</a>
              <!-- <br>
              *equal contribution,<span>&#9824;</span>project lead -->
              <br>
              <em>TMLR</em>, 2025
              <br>
              <a href='https://arxiv.org/abs/2408.03326' target='_blank'>PDF</a> / 
              <a href='https://llava-vl.github.io/blog/2024-08-05-llava-onevision/' target='_blank'>Dataset and Code</a> 
              <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/LLaVA-VL/LLaVA-NeXT?style=social"> 
              <p>  A family of LMMs developed by consolidating insights into data, models, and visual representations. </p>
            </td>
          </tr> 

          <tr>
            <td style="padding:20px;width:45%;vertical-align:middle">
              <div class="one">
                <img src='images/llava_interleaved.png' width="320" height="150">
              </div>
            </td>
            <td style="padding:20px;width:55%;vertical-align:middle">
              <a href="https://llava-vl.github.io/blog/2024-06-16-llava-next-interleave/">
                <papertitle>LLaVA-NeXT-Interleave: Tackling Multi-image, Video, and 3D in Large Multimodal Models</papertitle>
              </a>
              <br>
              
              <a href="https://fengli-ust.github.io/">Feng Li<sup>*</sup></a>,
              <a href="https://zrrskywalker.github.io/">Renrui Zhang<sup>*</sup></a>,
              <a href="https://scholar.google.com/citations?user=B8hPxMQAAAAJ&hl=en">Hao Zhang<sup>*</sup></a>,
              <strong>Yuanhan Zhang</strong>,
              <a href='https://brianboli.com/' target='_blank'>Bo Li</a>,
              <a href='https://scholar.google.com/citations?user=q8ZrKVIAAAAJ&hl=zh-CN' target='_blank'>Wei Li</a>,
              <a href='https://scholar.google.com/citations?user=XwY9LXoAAAAJ&hl=zh-CN' target='_blank'>Zejun Ma</a>,
              <br>
              <em>ICLR</em>, 2025 (Spotlight)
              <br>
              <a href='https://arxiv.org/pdf/2407.07895' target='_blank'>PDF</a> / 
              <a href='https://llava-vl.github.io/blog/2024-06-16-llava-next-interleave/' target='_blank'>Dataset and Code</a>
              <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/LLaVA-VL/LLaVA-NeXT?style=social"> 
              <p>  Tackling Multi-image, Video, and 3D in Large Multimodal Models. </p>
            </td>
          </tr>    

          <tr>
            <td style="padding:20px;width:45%;vertical-align:middle">
              <div class="one">
                <img src='images/mmbench.png' width="320" height="180">
              </div>
            </td>
            <td style="padding:20px;width:55%;vertical-align:middle">
              <a href="https://mmbench.opencompass.org.cn/home">
                <papertitle>MMBench: Is Your Multi-modal Model an All-around Player?</papertitle>
              </a>
              <br>
                <a href='https://yuanliuuuuuu.github.io/' target='_blank'>Yuan Liu<sup>*</sup></a>,
                <a href='https://kennymckormick.github.io/' target='_blank'>Haodong Duan<sup>*</sup></a>,
                <strong>Yuanhan Zhang<sup>*</sup></strong>,
                <a href='https://brianboli.com/' target='_blank'>Bo Li<sup>*</sup></a>,
                <a href='https://tonysy.github.io/' target='_blank'>Songyang Zhang<sup>*</sup></a>,
                <a href='https://scholar.google.com.hk/citations?user=aocj89kAAAAJ&hl=zh-CN' target='_blank'>Wangbo Zhao</a>,
                <a href='https://github.com/yyk-wew' target='_blank'>Yike Yuan</a>,
                <a href='https://myownskyw7.github.io/' target='_blank'>Jiaqi Wang</a>,
                <a href='https://conghui.github.io/' target='_blank'>Conghui He</a>,
                <a href='https://liuziwei7.github.io/' target='_blank'>Ziwei Liu</a>,
                <a href='https://chenkai.site/' target='_blank'>Kai Chen</a>,
                <a href='http://dahua.site/' target='_blank'>Dahua Lin</a>
              <br>
              <em>ECCV</em>, 2024 (Oral)
              <br>
              <a href='https://arxiv.org/abs/2307.06281' target='_blank'>PDF</a> / 
              <a href='https://mmbench.opencompass.org.cn/home' target='_blank'>Dataset and Code</a> 
              <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/open-compass/MMBench?style=social"> 
              <p>  Benchmarking the 20 abilities of vision-language models. </p>
            </td>
          </tr> 


          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

            <tr>
              <td style="padding:20px;width:45%;vertical-align:middle">
                <div class="one">
                  <img src='images/octopus_logo.png' width="320" height="200">
                </div>
              </td>
              <td style="padding:20px;width:55%;vertical-align:middle">
                <a href="https://choiszt.github.io/Octopus/">
                  <papertitle>Octopus: Embodied Vision-Language Programmer from Environmental Feedback</papertitle>
                </a>
                <br>
                <a href='https://jingkang50.github.io/' target='_blank'>Jingkang Yang</a>,
                <a href='https://github.com/dongyh20' target='_blank'>Yuhan Dong</a>,
                <a href='https://github.com/choiszt' target='_blank'>Shuai Liu</a>,
                <a href='https://brianboli.com/' target='_blank'>Bo Li</a>,
                Ziyue Wang, 
                Chencheng Jiang, 
                Haoran Tan, 
                Jiamu Kang,
                  <strong>Yuanhan Zhang</strong>,
                  <a href="https://kaiyangzhou.github.io/">Kaiyang Zhou</a>, 
                  <a href="https://liuziwei7.github.io/">Ziwei Liu</a>
                <br>
                <em>ECCV</em>, 2024
                <br>
                <a href='https://arxiv.org/abs/2310.08588' target='_blank'>PDF</a> / 
                <a href='https://choiszt.github.io/Octopus/' target='_blank'>Dataset and Code</a> 
                <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/dongyh20/Octopus?style=social"> 
                <p>   An embodied vision-language model trained with RLEF, emerging superior in embodied visual planning and programming. </p>
              </td>
            </tr> 


            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

              <tr>
                <td style="padding:20px;width:45%;vertical-align:middle">
                  <div class="one">
                    <img src='images/funqa_logo.png' width="320" height="180">
                  </div>
                </td>
                <td style="padding:20px;width:55%;vertical-align:middle">
                  <a href="https://funqa-benchmark.github.io/">
                    <papertitle>FunQA: Towards Surprising Video Comprehension</papertitle>
                  </a>
                  <br>
                    <a href='https://github.com/Nicous20' target='_blank'>Binzhu Xie</a>,
                    <a href='https://github.com/fesvhtr' target='_blank'>Sicheng Zhang</a>,
                    <a href='https://github.com/Zzitang' target='_blank'>Zitang Zhou</a>,
                    <a href='https://brianboli.com/' target='_blank'>Bo Li</a>,
                    <strong>Yuanhan Zhang</strong>,
                    <a href='https://jmhessel.com/' target='_blank'>Jack Hessel</a>,
                    <a href='https://jingkang50.github.io/' target='_blank'>Jingkang Yang</a>,
                    <a href='https://liuziwei7.github.io/' target='_blank'>Ziwei Liu</a>
                  <br>
                  <em>ECCV</em>, 2024
                  <br>
                  <a href='https://arxiv.org/abs/2306.14899' target='_blank'>PDF</a> / 
                  <a href='https://funqa-benchmark.github.io/' target='_blank'>Dataset and Code</a> 
                  <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/Nicous20/FunQA?style=social"> 
                  <p>  FunQA benchmarks funny, creative, and magic videos for challenging tasks. </p>
                </td>
              </tr> 

              <tr>
                <td style="padding:5px;width:45%;vertical-align:middle">
                  <div class="one">
                    <img src='https://i.postimg.cc/mksBCbV9/brand-title.png' width="360" height="140">
                  </div>
                </td>
                <td style="padding:20px;width:55%;vertical-align:middle">
                  <a href="https://github.com/Luodian/Otter">
                    <papertitle>Otter: A multi-modal model with in-context instruction tuning</papertitle>
                  </a>
                  <br>
                  <a href='https://brianboli.com/' target='_blank'>Bo Li<sup>*</sup></a>,
                  <strong>Yuanhan Zhang</strong><sup>*</sup>,
                  <!-- <strong>Yuanhan Zhang</strong><sup>*<span>&#9824;</span>, -->
                    <a href='https://cliangyu.com/' target='_blank'>Liangyu Chen</a>,
                    <a href='https://king159.github.io/' target='_blank'>Jinghao Wan</a>,
                    <a href='https://pufanyi.github.io/' target='_blank'>Fanyi Pu</a>,
                    <!-- </br> -->
                    <a href='https://jingkang50.github.io/' target='_blank'>Jingkang Yang</a>,
                    <a href='https://chunyuan.li/' target='_blank'>Chunyuan Li</a>,
                    <a href='https://liuziwei7.github.io/' target='_blank'>Ziwei Liu</a>
                  <!-- <br>
                  *equal contribution,<span>&#9824;</span>project lead -->
                  <br>
                  <em>TPAMI</em>
                  <br>
                  <a href='https://arxiv.org/abs/2306.05425' target='_blank'>PDF</a> / 
                  <a href='https://otter-ntu.github.io/' target='_blank'>Dataset and Code</a> 
                  <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/Luodian/Otter?style=social"> 
                  <p>  A vision-language model with in-context instruction tuning. </p>
                </td>
              </tr> 

          <tr>
            <td style="padding:20px;width:45%;vertical-align:middle">
              <div class="one">
                <img src='images/llm_animal.png' width="320" height="100">
              </div>
            </td>
            <td style="padding:20px;width:55%;vertical-align:middle">
              <a href="https://zhangyuanhan-ai.github.io/">
                <papertitle>Knowledge augmented instruction tuning for
                  zero-shot animal species recognition</papertitle>
              </a>
              <br>
              <a href=https://z-fabian.github.io/  target='_blank'>Zalan Fabian</a>,
              <a href=https://scholar.google.com/citations?user=at4m2mYAAAAJ&hl=en target='_blank'>Zhongqi Miao</a>,
              <a href='https://chunyuan.li/' target='_blank'>Chunyuan Li</a>,
              <strong>Yuanhan Zhang</strong>,
              <a href='https://liuziwei7.github.io/' target='_blank'>Ziwei Liu</a>,
              <a href='https://cider.uniandes.edu.co/en/professors/andres-hernandez-qui%C3%B1onez' target='_blank'>Andr√©s Hern√°ndez</a>,
              <a href='https://scholar.google.com/citations?user=J7eAM2sAAAAJ&hl=es' target='_blank'>Andr√©s Montes-Rojas</a>,
              <a href='https://www.linkedin.com/in/rafael-santiago-escucha-ram%C3%ADrez-a4149a227/?originalSubdomain=co' target='_blank'>Rafael Escucha</a>,
              <a href='https://orcid.org/0009-0005-9422-130X' target='_blank'>Laura Siabatto</a>, 
              <a href='https://scholar.google.es/citations?user=BkuODsEAAAAJ&hl=es' target='_blank'>Andr√©s Link</a>, 
              <a href='https://biomedicalcomputervision.uniandes.edu.co/' target='_blank'>Pablo Arbel√°ez</a>,
              <a href='https://www.microsoft.com/en-us/research/people/radodhia/Rahul Dodhiatarget=' target='_blank'>Rahul Dodhia</a>,
              <a href='https://www.microsoft.com/en-us/research/people/jlavista/target='_blank'> Juan Lavista Ferres</a>
              <br>
              <em>Instruction Tuning and Instruction Following Workshop@NeurIPS 2023.
              </em>
              <br>
              <br>
              <a href='https://arxiv.org/abs/2311.01064' target='_blank'>PDF</a>  
              <p>  A knowledge augmented vision-language model for AI conservation. </p>
            </td>
          </tr> 


          <tr>
            <td style="padding:20px;width:45%;vertical-align:middle">
              <div class="one">
                <img src='images/visual_prompt_retrieval.png' width="320" height="150">
              </div>
            </td>
            <td style="padding:20px;width:55%;vertical-align:middle">
              <a href="https://github.com/ZhangYuanhan-AI/visual_prompt_retrieval">
                <papertitle>What Makes Good Examples for Visual In-Context Learning?</papertitle>
              </a>
              <br>
              <strong>Yuanhan Zhang</strong>,
              <a href="https://kaiyangzhou.github.io/">Kaiyang Zhou</a>, 
              <a href="https://liuziwei7.github.io/">Ziwei Liu</a>
              <br>
              <em>NeurIPS</em>, 2023
              <br>
              <a href='https://arxiv.org/abs/2301.13670' target='_blank'>PDF</a> / 
              <a href='https://github.com/ZhangYuanhan-AI/visual_prompt_retrieval' target='_blank'>Code</a> 
              <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/ZhangYuanhan-AI/visual_prompt_retrieval?style=social"> 
              <p>  Retrieving prompt for visual in-context learning. </p>
            </td>
          </tr>    

          <tr>
            <td style="padding:20px;width:45%;vertical-align:middle">
              <div class="one">
                <img src='images/CIL.png' width="320" height="150">
              </div>
            </td>
            <td style="padding:20px;width:55%;vertical-align:middle">
              <a href="https://github.com/zhoudw-zdw/PROOF">
                <papertitle>Learning without Forgetting for Vision-Language Models</papertitle>
              </a>
              <br>
              <a href='http://www.lamda.nju.edu.cn/zhoudw' target='_blank'>Da-Wei Zhou</a>,
              <strong>Yuanhan Zhang</strong>;
              <a href=' ' target='_blank'>Yan Wang</a>,
              <a href='https://jingyinju.github.io/' target='_blank'>Jingyi Ning</a>,
              <a href='http://www.lamda.nju.edu.cn/yehj' target='_blank'>Han-Jia Ye</a>,
              <a href='http://www.lamda.nju.edu.cn/zhandc' target='_blank'>De-Chuan Zhan</a>,
              <a href='http://liuziwei7.github.io/' target='_blank'>Ziwei Liu</a>
              <br>
              <em>TPAMI</em>
              <br>
              <a href='https://arxiv.org/abs/2305.19270v1' target='_blank'>PDF</a> / 
              <a href='https://github.com/zhoudw-zdw/PROOF' target='_blank'>Code</a> 
              <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/zhoudw-zdw/PROOF?style=social"> 
              <p>  Learning without Forgetting for Vision-Language Models. </p>
            </td>
          </tr>    

          <tr onmouseout="noah_stop()" onmouseover="noah_start()">
            <td style="padding:20px;width:45%;vertical-align:middle">
              <div id="noah_anim" class="hidden" style="display: none;"><img src="images/NOAH_annimation.gif" width="320" height="160" style="border-style: none"></div>
              <div id="noah_still" style="display: inline;"><img src="images/NOAH_teaser.png" width="320" height="160" style="border-style: none"></div>
              <script type="text/javascript">
                function noah_start() {
                  document.getElementById('noah_anim').style.display = 'inline';
                  document.getElementById('noah_still').style.display = 'none';
                }
  
                function noah_stop() {
                  document.getElementById('noah_anim').style.display = 'none';
                  document.getElementById('noah_still').style.display = 'inline';
                }
                noah_stop()
              </script>
            </td>
            <td style="padding:20px;width:55%;vertical-align:middle">
              <a href="https://github.com/Davidzhangyuanhan/NOAH">
                <papertitle>Neural Prompt Search</papertitle>
              </a>
              <br>
              <strong>Yuanhan Zhang</strong>,
              <a href="https://kaiyangzhou.github.io/">Kaiyang Zhou</a>, 
              <a href="https://liuziwei7.github.io/">Ziwei Liu</a>
              <br>
              <em>TPAMI</em>  
              <br>
              <a href="https://arxiv.org/abs/2206.04673">PDF</a> / 
              <a href="https://zhangyuanhan-ai.github.io/NOAH">Project Page</a> / 
              <a href="https://github.com/Davidzhangyuanhan/NOAH">Code</a>  
              <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/Davidzhangyuanhan/NOAH?style=social">              
              <p> Searching prompt modules for parameter-efficient transfer learning.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:45%;vertical-align:middle">
              <div class="one">
                <img src='images/point_cloud_KD.png' width="320" height="180">
              </div>
            </td>
            <td style="padding:20px;width:55%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2212.08974">
                <papertitle>3D Point Cloud Pre-training with Knowledge Distillation from 2D Images?</papertitle>
              </a>
              <br>
              <a href="https://www.cs.rochester.edu/u/yyao39/#intro">Yuan Yao</a>,
              <strong>Yuanhan Zhang</strong>,
              <a href="https://yinzhenfei.github.io/">Zhenfei Yin</a>, 
              <a href="https://cs.rochester.edu/u/jluo/">Jiebo Luo</a>,
              <a href="https://wlouyang.github.io/">Wanli Ouyang</a>,
              <a href="https://xiaoshuihuang.github.io/">Xiaoshui Huang</a>.
              <br>
              <em>ICME</em>, 2023
              <br>
              <a href='https://arxiv.org/abs/2212.08974' target='_blank'>PDF</a> / 
              <a href='https://arxiv.org/abs/2212.08974' target='_blank'>Code</a> 
              <p>  3D Point Cloud Pre-training with Knowledge Distillation from 2D Images. </p>
            </td>
          </tr> 


          <tr>
            <td style="padding:20px;width:45%;vertical-align:middle">
              <div class="one">
                <img src='images/OmniBenchmark.png' width="300" height="180">
              </div>
            </td>
            <td style="padding:20px;width:55%;vertical-align:middle">
              <a href="https://zhangyuanhan-ai.github.io/OmniBenchmark">
                <papertitle>Benchmarking Omni-Vision Representation through the Lens of Visual Realms</papertitle>
              </a>
              <br>
              <strong>Yuanhan Zhang</strong>, 
              <a href="https://yinzhenfei.github.io/">Zhenfei Yin</a>, 
              <a href="https://amandajshao.github.io/">Jing Shao</a>, 
              <a href="https://liuziwei7.github.io/">Ziwei Liu</a>
              <br>
              <em>ECCV</em>, 2022
              <br>
              <a href='https://arxiv.org/abs/2207.07106' target='_blank'>PDF</a> / 
              <a href='https://zhangyuanhan-ai.github.io/OmniBenchmark' target='_blank'>Project Page</a> /  
              <a href='https://paperswithcode.com/sota/image-classification-on-omnibenchmark' target='_blank'>Leaderboard</a> / 
              <a href='https://codalab.lisn.upsaclay.fr/competitions/6043' target='_blank'>Challenge:ImageNet1k-Pretrain Track</a> / 
              <!-- <br> -->
              <a href='https://codalab.lisn.upsaclay.fr/competitions/6045' target='_blank'>Challenge:Open-Pretrain Track</a> /
              <a href='https://github.com/ZhangYuanhan-AI/OmniBenchmark' target='_blank'>Dataset and Code</a> 
              <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/ZhangYuanhan-AI/OmniBenchmark?style=social"> 
              <p>  New benchmark for evaluating vision foundation models; New supervised contrastive learning framework. </p>
            </td>
          </tr>    
        
          <tr>
            <td style="padding:20px;width:45%;vertical-align:middle">
              <div class="one">
                <img src='images/bamboo_teaser_annimation.gif' width="320" height="160">
              </div>
            </td>
            <td style="padding:20px;width:55%;vertical-align:middle">
              <a href="https://opengvlab.shlab.org.cn/bamboo/home">
                <papertitle>Bamboo: Building Mega-Scale Vision Dataset Continually with Human-Machine Synergy</papertitle>
              </a>
              <br>
              <strong>Yuanhan Zhang</strong>,
              Qinghong Sun, Yichun Zhou, Zexin He,
              <br> 
              <a href="https://yinzhenfei.github.io/">Zhenfei Yin</a>,
              Kun Wang,
              <a href="https://lucassheng.github.io/">Lu Sheng</a>, 
              <a href="http://mmlab.siat.ac.cn/yuqiao">Yu Qiao</a>,
              <a href="https://amandajshao.github.io/">Jing Shao</a>, 
              <a href="https://liuziwei7.github.io/">Ziwei Liu</a>
              <br>
              <em>IJCV</em>, 2025  
              <br>
              <a href="https://arxiv.org/abs/2203.07845">PDF</a> / 
              <a href="https://opengvlab.shlab.org.cn/bamboo/home">Project Page</a> / 
              <a href="https://huggingface.co/spaces/CVPR/Bamboo_ViT-B16_demo">Demo</a> /
              <a href="https://github.com/Davidzhangyuanhan/Bamboo">Code</a> 
              <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/Davidzhangyuanhan/Bamboo?style=social"> 
              <p></p>
              
              <p> 4 times larger than ImageNet; 2 time larger than Object365; Built by active learning.</p>
            </td>
          </tr>
      
          <tr>
            <td style="padding:20px;width:45%;vertical-align:middle">
              <div class="one">
                <img src='images/spoofchallenge_logo.png' width="320" height="160">
              </div>
            </td>
            <td style="padding:20px;width:55%;vertical-align:middle">
							<a href="https://github.com/Davidzhangyuanhan/CelebA-Spoof">
                <papertitle>CelebA-Spoof: Large-Scale Face Anti-Spoofing Dataset with Rich Annotations</papertitle>
              </a>
              <br>
              <strong>Yuanhan Zhang</strong>, 
              <a href="https://yinzhenfei.github.io/">Zhenfei Yin</a>,
              <a href="https://faculty.bjtu.edu.cn/8408/">Yidong Li</a>, 
              <a href="https://gjyin91.github.io/">Guojun Yin</a>, 
              <a href="https://yan-junjie.github.io/">Junjie Yan</a>, 
              <a href="https://amandajshao.github.io/">Jing Shao</a>, 
              <a href="https://liuziwei7.github.io/">Ziwei Liu</a>
              <br>
              <em>ECCV</em>, 2020
              <br>
              <a href="https://arxiv.org/abs/2007.12342">PDF</a> /
              <a href="https://mmlab.ie.cuhk.edu.hk/projects/CelebA/CelebA_Spoof.html">Dataset</a> /
              <a href="https://www.youtube.com/watch?v=A7XjSg5srvI">Demo</a> /
              <a href="https://github.com/Davidzhangyuanhan/CelebA-Spoof">Code</a> 
              <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/Davidzhangyuanhan/CelebA-Spoof?style=social">
              <p></p>
              <p>  Large-scale face-antispoofing Dataset. </p>
            </td>
          </tr>

        </tbody></table>
				
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading>Activities</heading>
          </td>
          </tr>
        </tbody></table>

        <ul>
          <li>
            Reviewer: CVPR/ECCV/ICCV/ICLR/NeurIPS/TPAMI/IJCV
          </li>

          <li>
            Organizing Committee: <a href="https://theaitalks.org/">The AI Talk</a>, CVPR24 Workshop(<a hred="https://prompting-in-vision.github.io/index_cvpr24.html">Prompting in Vision</a>)
            ECCV22 Workshop(<a href="https://sense-human.github.io/">workshop1</a>, <a href="https://computer-vision-in-the-wild.github.io/eccv-2022/">workshop2</a>)
          </li>
        </ul>


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading>Public Office Hour</heading>
          </td>
          </tr>
        </tbody></table>

        <ul>
          <li>
            <!-- Calendly link widget begin -->
            <link href="https://assets.calendly.com/assets/external/widget.css" rel="stylesheet">
            <script src="https://assets.calendly.com/assets/external/widget.js" type="text/javascript" async></script>
            <a href="" onclick="Calendly.initPopupWidget({url: 'https://calendly.com/zhangyuanhan/15min'});return false;">Happy to chat about any topics :)</a>
            <!-- Calendly link widget end -->
          </li>
       </ul>




        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                Last updated in Aug. 2025. </a>
              </p>
              <p style="text-align:right;font-size:small;">
                Homepage credits: <a href="https://github.com/jonbarron/jonbarron_website">Jon Barron.</a>
              </p>
            </td>
          </tr>
        </tbody></table>

      </td>
    </tr>
  </table>
</body>

</html>
