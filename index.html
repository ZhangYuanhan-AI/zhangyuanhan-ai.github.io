<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Yuanhan Zhang</title>
  
  <meta name="author" content="Yuanhan Zhang">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
	<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üåê</text></svg>">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
          <tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p style="text-align:center">
                  <name>Yuanhan Zhang (Âº† ÂÖÉÁÄö)</name> 
                </p>
                <p>Hi! I'm Yuanhan Zhang, here is the standard Chinese pronunciation for my first name : <a href="https://translate.google.com/?hl=zh-CN&sl=auto&tl=zh-CN&text=%E5%85%83%E7%80%9A&op=translate">Yuanhan</a>, a second-year PhD student at <a href="https://www.mmlab-ntu.com/">MMLab@NTU</a>, supervised by Prof. <a href="https://liuziwei7.github.io/">Ziwei Liu</a>. My research interests lie on the computer vision and deep learning. Particularly, I am interested in multi-modal learning and its transferable ability.
                </p>
                <p style="text-align:center">
                  <a href="mailto:yuanhan002@e.ntu.edu.sg">Email</a> &nbsp/&nbsp
                  <a href="https://scholar.google.com/citations?user=g6grFy0AAAAJ&hl=zh-CN">Google Scholar</a> &nbsp/&nbsp
                  <a href="https://twitter.com/zhang_yuanhan">Twitter</a> &nbsp/&nbsp
                  <a href="https://github.com/ZhangYuanhan-AI">Github</a>
                </p>
              </td>
              <td style="padding:2.5%;width:40%;max-width:40%">
                <a href="images/zyh.jpeg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/zyh_circle2.png" class="hoverZoomLink"></a>
              </td>
            </tr>
          </tbody>
        </table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>News</heading>
            </td>
          </tr>
        </tbody></table>

        <ul>
          <li>
            [2023-09] <a href="https://github.com/ZhangYuanhan-AI/visual_prompt_retrieval">Visual Prompt Retrieval</a> is accepted in NeurIPS2023, see you in New Orleans!
          </li>
          <li>
            [2023-09] Gave a talk at Alibaba, Damo Academy, Hosted by Dr. <a href="https://lidongbing.github.io/">Lidong Bin</a>
          </li>
          <li>
            [2023-09] Gave a talk at HITSZ, Hosted by Prof. <a href="https://rshaojimmy.github.io/">Rui Shao</a>
          </li>
          <li>
            [2023-06] Introducing <a href="https://otter-ntu.github.io/">Otter</a>. Check it out now!
          </li>
          <li>
            [2022-10] We won the first place in <a href="https://computer-vision-in-the-wild.github.io/eccv-2022/">Computer Vision in the Wild Challenge</a>.
          </li>
          <li>
            [2022-07] <a href="https://zhangyuanhan-ai.github.io/OmniBenchmark">OmniBenchmark</a> is accepted in ECCV2022.
          </li>
          <li>
            [2022-06] Try out the Bamboo demo on Huggingface spaces <a href="https://huggingface.co/spaces/CVPR/Bamboo_ViT-B16_demo" rel="nofollow"><img src="https://camo.githubusercontent.com/00380c35e60d6b04be65d3d94a58332be5cc93779f630bcdfc18ab9a3a7d3388/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f25463025394625413425393725323048756767696e67253230466163652d5370616365732d626c7565" alt="Hugging Face Spaces" data-canonical-src="https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue" style="max-width: 100%;"></a> 
          </li>
          <li>
            [2022-03] <a href="https://github.com/Davidzhangyuanhan/Bamboo">Bamboo</a> dataset released.
          </li>
        </ul>


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading>Pre-prints</heading>
          </td>
        </tr>
      </tbody></table>

      <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

        <tr>
          <td style="padding:20px;width:45%;vertical-align:middle">
            <div class="one">
              <img src='images/mmbench.png' width="320" height="180">
            </div>
          </td>
          <td style="padding:20px;width:55%;vertical-align:middle">
            <a href="https://mmbench.opencompass.org.cn/home">
              <papertitle>MMBench: Is Your Multi-modal Model an All-around Player?</papertitle>
            </a>
            <br>
              <a href='https://yuanliuuuuuu.github.io/' target='_blank'>Yuan Liu</a>,
              <a href='https://kennymckormick.github.io/' target='_blank'>Haodong Duan</a>,
              <strong>Yuanhan Zhang</strong>,
              <a href='https://brianboli.com/' target='_blank'>Bo Li</a>,
              <a href='https://tonysy.github.io/' target='_blank'>Songyang Zhang</a>,
              <a href='https://scholar.google.com.hk/citations?user=aocj89kAAAAJ&hl=zh-CN' target='_blank'>Wangbo Zhao</a>,
              <a href='https://github.com/yyk-wew' target='_blank'>Yike Yuan</a>,
              <a href='https://myownskyw7.github.io/' target='_blank'>Jiaqi Wang</a>,
              <a href='https://conghui.github.io/' target='_blank'>Conghui He</a>,
              <a href='https://liuziwei7.github.io/' target='_blank'>Ziwei Liu</a>,
              <a href='https://chenkai.site/' target='_blank'>Kai Chen</a>,
              <a href='http://dahua.site/' target='_blank'>Dahua Lin</a>
            <br>
            <em>arXiv Preprint</em>, 2023
            <br>
            <a href='https://arxiv.org/abs/2307.06281' target='_blank'>PDF</a> / 
            <a href='https://mmbench.opencompass.org.cn/home' target='_blank'>Dataset and Code</a> 
            <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/open-compass/MMBench?style=social"> 
            <p>  Benchmarking the multi-modal understanding capability of vision language models </p>
          </td>
        </tr> 

        <tr>
          <td style="padding:5px;width:45%;vertical-align:middle">
            <div class="one">
              <img src='https://i.postimg.cc/mksBCbV9/brand-title.png' width="360" height="140">
            </div>
          </td>
          <td style="padding:20px;width:55%;vertical-align:middle">
            <a href="https://github.com/Luodian/Otter">
              <papertitle>Otter: A multi-modal model with in-context instruction tuning</papertitle>
            </a>
            <br>
            <a href='https://brianboli.com/' target='_blank'>Bo Li<sup>*<span>&#9824;</span></sup></a>,
            <strong>Yuanhan Zhang</strong><sup>*<span>&#9824;</span>,
              <a href='https://cliangyu.com/' target='_blank'>Liangyu Chen<sup>*</sup></a>,
              <a href='https://king159.github.io/' target='_blank'>Jinghao Wang<sup>*</sup></a>,
              <a href='https://pufanyi.github.io/' target='_blank'>Fanyi Pu<sup>*</sup></a>,
              <!-- </br> -->
              <a href='https://jingkang50.github.io/' target='_blank'>Jingkang Yang</a>,
              <a href='https://chunyuan.li/' target='_blank'>Chunyuan Li</a>,
              <a href='https://liuziwei7.github.io/' target='_blank'>Ziwei Liu</a>
            <br>
            *equal contribution,<span>&#9824;</span>project lead
            <br>
            <em>arXiv Preprint</em>, 2023
            <br>
            <a href='https://arxiv.org/abs/2306.05425' target='_blank'>PDF</a> / 
            <a href='https://otter-ntu.github.io/' target='_blank'>Dataset and Code</a> 
            <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/Luodian/Otter?style=social"> 
            <p>  A vision-language model with in-context instruction tuning </p>
          </td>
        </tr>    

        <tr onmouseout="noah_stop()" onmouseover="noah_start()">
          <td style="padding:20px;width:45%;vertical-align:middle">
            <div id="noah_anim" class="hidden" style="display: none;"><img src="images/NOAH_annimation.gif" width="320" height="160" style="border-style: none"></div>
            <div id="noah_still" style="display: inline;"><img src="images/NOAH_teaser.png" width="320" height="160" style="border-style: none"></div>
            <script type="text/javascript">
              function noah_start() {
                document.getElementById('noah_anim').style.display = 'inline';
                document.getElementById('noah_still').style.display = 'none';
              }

              function noah_stop() {
                document.getElementById('noah_anim').style.display = 'none';
                document.getElementById('noah_still').style.display = 'inline';
              }
              noah_stop()
            </script>
          </td>
          <td style="padding:20px;width:55%;vertical-align:middle">
            <a href="https://github.com/Davidzhangyuanhan/NOAH">
              <papertitle>Neural Prompt Search</papertitle>
            </a>
            <br>
            <strong>Yuanhan Zhang</strong>,
            <a href="https://kaiyangzhou.github.io/">Kaiyang Zhou</a>, 
            <a href="https://liuziwei7.github.io/">Ziwei Liu</a>.
            <br>
            <em>arXiv Preprint</em>, 2022  
            <br>
            <a href="https://arxiv.org/abs/2206.04673">PDF</a> / 
            <a href="https://zhangyuanhan-ai.github.io/NOAH">Project Page</a> / 
            <a href="https://github.com/Davidzhangyuanhan/NOAH">Code</a>  
            <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/Davidzhangyuanhan/NOAH?style=social">              
            <p> Searching prompt modules for parameter-efficient transfer learning.</p>
          </td>
        </tr>

        <tr>
          <td style="padding:20px;width:45%;vertical-align:middle">
            <div class="one">
              <img src='images/bamboo_teaser_annimation.gif' width="320" height="160">
            </div>
          </td>
          <td style="padding:20px;width:55%;vertical-align:middle">
            <a href="https://opengvlab.shlab.org.cn/bamboo/home">
              <papertitle>Bamboo: Building Mega-Scale Vision Dataset Continually with Human-Machine Synergy</papertitle>
            </a>
            <br>
            <strong>Yuanhan Zhang</strong>,
            Qinghong Sun, Yichun Zhou, Zexin He,
            <br> Zhenfei Yin, Kun Wang,
            <a href="https://lucassheng.github.io/">Lu Sheng</a>, 
            <a href="http://mmlab.siat.ac.cn/yuqiao">Yu Qiao</a>,
            <a href="https://amandajshao.github.io/">Jing Shao</a>, 
            <a href="https://liuziwei7.github.io/">Ziwei Liu</a>.
            <br>
            <em>arXiv Preprint</em>, 2022  
            <br>
            <a href="https://arxiv.org/abs/2203.07845">PDF</a> / 
            <a href="https://opengvlab.shlab.org.cn/bamboo/home">Project Page</a> / 
            <a href="https://huggingface.co/spaces/CVPR/Bamboo_ViT-B16_demo">Demo</a> /
            <a href="https://github.com/Davidzhangyuanhan/Bamboo">Code</a> 
            <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/Davidzhangyuanhan/Bamboo?style=social"> 
            <p></p>
            
            <p> 4 times larger than ImageNet; 2 time larger than Object365; Built by active learning.</p>
          </td>
        </tr>


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Publications</heading>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>



          <tr>
            <td style="padding:20px;width:45%;vertical-align:middle">
              <div class="one">
                <img src='images/llm_animal.png' width="320" height="110">
              </div>
            </td>
            <td style="padding:20px;width:55%;vertical-align:middle">
              <a href="https://zhangyuanhan-ai.github.io/">
                <papertitle>Knowledge augmented instruction tuning for
                  zero-shot animal species recognition</papertitle>
              </a>
              <br>
              <a href=https://z-fabian.github.io/  target='_blank'>Zalan Fabian</a>,
              <a href=https://scholar.google.com/citations?user=at4m2mYAAAAJ&hl=en target='_blank'>Zhongqi Miao</a>,
              <a href='https://chunyuan.li/' target='_blank'>Chunyuan Li</a>,
              <strong>Yuanhan Zhang</strong><sup><span>&circ;</span></sup>,
              <a href='https://liuziwei7.github.io/' target='_blank'>Ziwei Liu</a>,
              Andr√©s Hern√°ndez</a>,
              <a href='https://scholar.google.com/citations?user=J7eAM2sAAAAJ&hl=es' target='_blank'>Andr√©s Montes-Rojas</a>,
              Rafael Escucha</a>,
              Laura Siabatto</a>, 
              <a href='https://scholar.google.es/citations?user=BkuODsEAAAAJ&hl=es' target='_blank'>Andr√©s Link</a>, 
              <a href='https://biomedicalcomputervision.uniandes.edu.co/' target='_blank'>Pablo Arbel√°ez</a>,
              <a href='https://www.microsoft.com/en-us/research/people/radodhia/Rahul Dodhiatarget=' target='_blank'>Rahul Dodhia</a>,
              <a href='https://www.microsoft.com/en-us/research/people/jlavista/target='_blank'> Juan Lavista Ferres</a>
              <br>
              <em>Instruction Tuning and Instruction Following Workshop@NeurIPS 2023.
              </em>
              <br>
              <a href='https://arxiv.org/abs/2311.01064' target='_blank'>PDF</a>  
              <p>  A knowledge augmented vision-language model for AI conservation </p>
            </td>
          </tr> 


          <tr>
            <td style="padding:20px;width:45%;vertical-align:middle">
              <div class="one">
                <img src='images/visual_prompt_retrieval.png' width="320" height="180">
              </div>
            </td>
            <td style="padding:20px;width:55%;vertical-align:middle">
              <a href="https://github.com/ZhangYuanhan-AI/visual_prompt_retrieval">
                <papertitle>What Makes Good Examples for Visual In-Context Learning?</papertitle>
              </a>
              <br>
              <strong>Yuanhan Zhang</strong>,
              <a href="https://kaiyangzhou.github.io/">Kaiyang Zhou</a>, 
              <a href="https://liuziwei7.github.io/">Ziwei Liu</a>.
              <br>
              <em>NeurIPS</em>, 2023
              <br>
              <a href='https://arxiv.org/abs/2301.13670' target='_blank'>PDF</a> / 
              <a href='https://github.com/ZhangYuanhan-AI/visual_prompt_retrieval' target='_blank'>Code</a> 
              <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/ZhangYuanhan-AI/visual_prompt_retrieval?style=social"> 
              <p>  Retrieve prompt for visual in-context learning. </p>
            </td>
          </tr>    




          <tr>
            <td style="padding:20px;width:45%;vertical-align:middle">
              <div class="one">
                <img src='images/OmniBenchmark.png' width="320" height="180">
              </div>
            </td>
            <td style="padding:20px;width:55%;vertical-align:middle">
              <a href="https://zhangyuanhan-ai.github.io/OmniBenchmark">
                <papertitle>Benchmarking Omni-Vision Representation through the Lens of Visual Realms</papertitle>
              </a>
              <br>
              <strong>Yuanhan Zhang</strong>, 
              Zhenfei Yin, 
              <a href="https://amandajshao.github.io/">Jing Shao</a>, 
              <a href="https://liuziwei7.github.io/">Ziwei Liu</a>.
              <br>
              <em>ECCV</em>, 2022
              <br>
              <a href='https://arxiv.org/abs/2207.07106' target='_blank'>PDF</a> / 
              <a href='https://zhangyuanhan-ai.github.io/OmniBenchmark' target='_blank'>Project Page</a> /  
              <a href='https://paperswithcode.com/sota/image-classification-on-omnibenchmark' target='_blank'>Leaderboard</a> / 
              <a href='https://codalab.lisn.upsaclay.fr/competitions/6043' target='_blank'>Challenge:ImageNet1k-Pretrain Track</a> / 
              <!-- <br> -->
              <a href='https://codalab.lisn.upsaclay.fr/competitions/6045' target='_blank'>Challenge:Open-Pretrain Track</a> /
              <a href='https://github.com/ZhangYuanhan-AI/OmniBenchmark' target='_blank'>Dataset and Code</a> 
              <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/ZhangYuanhan-AI/OmniBenchmark?style=social"> 
              <p>  New benchmark for evaluating pre-trained model; New supervised contrastive learning framework. </p>
            </td>
          </tr>    
        
				
      
          <tr>
            <td style="padding:20px;width:45%;vertical-align:middle">
              <div class="one">
                <img src='images/spoofchallenge_logo.png' width="320" height="160">
              </div>
            </td>
            <td style="padding:20px;width:55%;vertical-align:middle">
							<a href="https://github.com/Davidzhangyuanhan/CelebA-Spoof">
                <papertitle>CelebA-Spoof: Large-Scale Face Anti-Spoofing Dataset with Rich Annotations</papertitle>
              </a>
              <br>
              <strong>Yuanhan Zhang</strong>, 
              Zhenfei Yin, 
              <a href="https://faculty.bjtu.edu.cn/8408/">Yidong Li</a>, 
              <a href="https://gjyin91.github.io/">Guojun Yin</a>, 
              <a href="https://yan-junjie.github.io/">Junjie Yan</a>, 
              <a href="https://amandajshao.github.io/">Jing Shao</a>, 
              <a href="https://liuziwei7.github.io/">Ziwei Liu</a>.
              <br>
              <em>ECCV</em>, 2020
              <br>
              <a href="https://arxiv.org/abs/2007.12342">PDF</a> /
              <a href="https://mmlab.ie.cuhk.edu.hk/projects/CelebA/CelebA_Spoof.html">Dataset</a> /
              <a href="https://www.youtube.com/watch?v=A7XjSg5srvI">Demo</a> /
              <a href="https://github.com/Davidzhangyuanhan/CelebA-Spoof">Code</a> 
              <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/Davidzhangyuanhan/CelebA-Spoof?style=social">
              <p></p>
              <p>  Large-scale face-antispoofing Dataset. </p>
            </td>
          </tr>

        </tbody></table>
				
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading>Activities</heading>
          </td>
          </tr>
        </tbody></table>

        <ul>
          <li>
            Conference reviewer for CVPR/ECCV/ICCV/ICLR/NeurIPS
          </li>

          <li>
            Journal reviewer for T-PAMI/IJCV
          </li>

          <li>
            Organizer for <a href="https://theaitalks.org/">The AI Talk</a>
          </li>
        </ul>


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading>Public Office Hour</heading>
          </td>
          </tr>
        </tbody></table>

        <ul>
          <li>
            <!-- Calendly link widget begin -->
            <link href="https://assets.calendly.com/assets/external/widget.css" rel="stylesheet">
            <script src="https://assets.calendly.com/assets/external/widget.js" type="text/javascript" async></script>
            <a href="" onclick="Calendly.initPopupWidget({url: 'https://calendly.com/zhangyuanhan/15min'});return false;">Happy to chat about any topics :)</a>
            <!-- Calendly link widget end -->
          </li>
       </ul>




        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                Last updated in Dec. 2023. </a>
              </p>
              <p style="text-align:right;font-size:small;">
                Homepage credits: <a href="https://github.com/jonbarron/jonbarron_website">Jon Barron.</a>
              </p>
            </td>
          </tr>
        </tbody></table>

      </td>
    </tr>
  </table>
</body>

</html>
