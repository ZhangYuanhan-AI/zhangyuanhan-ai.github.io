<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <title>Yuanhan Zhang</title>

  <meta name="author" content="Yuanhan (John) Zhang" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="description" content="Yuanhan (John) Zhang ‚Äî PhD student at MMLab@NTU. Research: computer vision, multimodal learning, benchmarking, and model adaptation." />

  <!-- Open Graph / Twitter (optional but helpful) -->
  <meta property="og:title" content="Yuanhan Zhang" />
  <meta property="og:description" content="PhD student at MMLab@NTU. Research: computer vision, multimodal learning, benchmarking, and model adaptation." />
  <meta property="og:type" content="website" />

  <link rel="stylesheet" type="text/css" href="stylesheet.css" />
  <link
    rel="icon"
    href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üåê</text></svg>"
  />
</head>

<body>
  <table style="width:100%;max-width:800px;border:0;border-spacing:0;border-collapse:separate;margin:0 auto;">
    <tbody>
      <tr>
        <td style="padding:0;">
          <!-- Header -->
          <table style="width:100%;border:0;border-spacing:0;border-collapse:separate;margin:0 auto;">
            <tbody>
              <tr>
                <td style="padding:2.5%;width:63%;vertical-align:middle;">
                  <p style="text-align:center;">
                    <name role="heading" aria-level="1">Yuanhan (John) Zhang</name>
                  </p>
                  <p>
                    Hi! I'm Yuanhan Zhang; here is the standard Chinese pronunciation of my given name:
                    <a href="https://translate.google.com/?hl=zh-CN&sl=auto&tl=zh-CN&text=%E5%85%83%E7%80%9A&op=translate" target="_blank" rel="noopener noreferrer">Yuanhan</a>.
                    I am a third-year PhD student at
                    <a href="https://www.mmlab-ntu.com/" target="_blank" rel="noopener noreferrer">MMLab@NTU</a>,
                    supervised by Prof.
                    <a href="https://liuziwei7.github.io/" target="_blank" rel="noopener noreferrer">Ziwei Liu</a>.
                  </p>
                  <p>
                    My research interests lie in computer vision and deep learning. I focus on adapting foundation models‚Äîfrom vision to
                    multimodal‚Äîfor real-world use: benchmarking model performance and adapting models via parameter-efficient tuning,
                    in-context learning, and instruction tuning.
                  </p>
                  <p style="text-align:center;">
                    <a href="mailto:yuanhan002@e.ntu.edu.sg">Email</a> (yuanhan002@e.ntu.edu.sg) &nbsp;/&nbsp;
                    <a href="https://scholar.google.com/citations?user=g6grFy0AAAAJ&hl" target="_blank" rel="noopener noreferrer">Google Scholar</a> &nbsp;/&nbsp;
                    <a href="https://twitter.com/zhang_yuanhan" target="_blank" rel="noopener noreferrer">Twitter</a> &nbsp;/&nbsp;
                    <a href="https://github.com/ZhangYuanhan-AI" target="_blank" rel="noopener noreferrer">GitHub</a>
                  </p>
                </td>
                <td style="padding:2.5%;width:40%;max-width:40%;vertical-align:middle;">
                  <a href="images/zyh.jpeg" target="_blank" rel="noopener noreferrer">
                    <img
                      src="images/zyh_circle2.png"
                      alt="Portrait of Yuanhan Zhang"
                      style="width:100%;max-width:100%;"
                      class="hoverZoomLink"
                      loading="lazy"
                    />
                  </a>
                </td>
              </tr>
            </tbody>
          </table>

          <!-- News -->
          <table style="width:100%;border:0;border-spacing:0;border-collapse:separate;margin:0 auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle;">
                  <heading role="heading" aria-level="2">News</heading>
                </td>
              </tr>
            </tbody>
          </table>

          <ul>
            <li>
              [2025-07] We release Video Thinking Test (üìΩÔ∏è
              <a href="https://zhangyuanhan-ai.github.io/video-tt/" target="_blank" rel="noopener noreferrer">Video-TT</a>
              üìΩÔ∏è), a holistic benchmark to assess advanced reasoning and understanding correctness/robustness between MLLMs and humans.
            </li>
            <details>
              <summary>Older News &amp; Activities</summary>
              <ul>
                <li>
                  [2024-10] We update
                  <a href="https://llava-vl.github.io/blog/2024-09-30-llava-video/" target="_blank" rel="noopener noreferrer">LLaVA-Video</a>
                  (formerly LLaVA-NeXT-Video), releasing both the
                  <a href="https://huggingface.co/collections/lmms-lab/llava-video-661e86f5e8dabc3ff793c944" target="_blank" rel="noopener noreferrer">model</a>
                  and the
                  <a href="https://huggingface.co/datasets/lmms-lab/LLaVA-Video-178K" target="_blank" rel="noopener noreferrer">data</a>.
                </li>
                <li>
                  [2024-08]
                  <a href="https://llava-vl.github.io/blog/2024-08-05-llava-onevision/" target="_blank" rel="noopener noreferrer">LLaVA-OneVision</a> released ‚Äî an LMM that excels across single-image, multi-image, and video tasks.
                </li>
                <li>[2024-07] IJCV Outstanding Reviewer Award 2023.</li>
                <li>[2024-07] <a href="https://zhangyuanhan-ai.github.io/NOAH" target="_blank" rel="noopener noreferrer">NOAH</a> accepted by TPAMI.</li>
                <li>[2024-07] Three papers accepted at ECCV 2024.</li>
                <li>[2024-06] Organized CVPR 2024 workshop:
                  <a href="https://prompting-in-vision.github.io/index_cvpr24.html" target="_blank" rel="noopener noreferrer">Prompting in Vision</a>.
                </li>
                <li>[2024-05]
                  <a href="https://github.com/LLaVA-VL/LLaVA-NeXT" target="_blank" rel="noopener noreferrer">LLaVA-NeXT-Video</a> released.
                </li>
                <li>[2023-09]
                  <a href="https://github.com/ZhangYuanhan-AI/visual_prompt_retrieval" target="_blank" rel="noopener noreferrer">Visual Prompt Retrieval</a>
                  accepted to NeurIPS 2023.
                </li>
                <li>[2023-09] Talk at Alibaba DAMO Academy, hosted by Dr.
                  <a href="https://lidongbing.github.io/" target="_blank" rel="noopener noreferrer">Lidong Bin</a>.
                </li>
                <li>[2023-07] Talk at HITSZ, hosted by Prof.
                  <a href="https://rshaojimmy.github.io/" target="_blank" rel="noopener noreferrer">Rui Shao</a>.
                </li>
                <li>[2023-06] Introducing
                  <a href="https://otter-ntu.github.io/" target="_blank" rel="noopener noreferrer">Otter</a>.
                </li>
                <li>[2022-10] 1st place in
                  <a href="https://computer-vision-in-the-wild.github.io/eccv-2022/" target="_blank" rel="noopener noreferrer">Computer Vision in the Wild Challenge</a>.
                </li>
                <li>[2022-07]
                  <a href="https://zhangyuanhan-ai.github.io/OmniBenchmark" target="_blank" rel="noopener noreferrer">OmniBenchmark</a> accepted to ECCV 2022.
                </li>
                <li>[2022-03]
                  <a href="https://github.com/Davidzhangyuanhan/Bamboo" target="_blank" rel="noopener noreferrer">Bamboo</a> dataset released.
                </li>
              </ul>
            </details>
          </ul>

          <!-- Publications -->
          <table style="width:100%;border:0;border-spacing:0;border-collapse:separate;margin:0 auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle;">
                  <heading role="heading" aria-level="2">Publications</heading>
                </td>
              </tr>
            </tbody>
          </table>

          <table style="width:100%;border:0;border-spacing:0;border-collapse:separate;margin:0 auto;">
            <tbody>
              
              <!-- Video-TT -->
              <tr>
                  <td style="padding:5px 12px 5px 30px; width:300px; vertical-align:top;">                  
                    <div class="one">
                    <img src="images/video-tt.png" width="260" alt="Video-TT teaser" loading="lazy" style="display:block;"/>
                  </div>
                </td>
                  <td style="padding:5px 0 5px 0; vertical-align:top;">                  
                    <a href="https://zhangyuanhan-ai.github.io/video-tt/" target="_blank" rel="noopener noreferrer">
                    <papertitle>Towards Video Thinking Test: A Holistic Benchmark for Advanced Video Reasoning and Understanding</papertitle>
                  </a>
                  <br />
                  <strong>Yuanhan Zhang<sup>*</sup></strong>,
                  Yunice Chew<sup>*</sup>,
                  <a href="https://scholar.google.com/citations?user=kMui170AAAAJ&hl=zh-CN" target="_blank" rel="noopener noreferrer">Yuhao Dong</a>,
                  Aria Leo,
                  Bo Hu,
                  <a href="https://liuziwei7.github.io/" target="_blank" rel="noopener noreferrer">Ziwei Liu</a>
                  <br />
                  <em>ICCV</em>, 2025
                  <br />
                  <a href="https://arxiv.org/abs/2507.15028" target="_blank" rel="noopener noreferrer">PDF</a> /
                  <a href="https://huggingface.co/datasets/lmms-lab/video-tt" target="_blank" rel="noopener noreferrer">Dataset and Code</a>
                  <p>A holistic benchmark to assess advanced reasoning and understanding correctness/robustness between MLLMs and humans.</p>
                </td>
              </tr>

              <tr>
                <td colspan="2" style="height:20px;"></td>
              </tr>
              <!-- LLaVA-Video -->
              <tr>
                  <td style="padding:5px 12px 5px 30px; width:300px; vertical-align:top;">                  
                    <div class="one">
                    <img src="images/llava-video.jpg"  width="260"  alt="LLaVA-Video teaser" loading="lazy" />
                  </div>
                </td>
                  <td style="padding:5px 0 5px 0; vertical-align:top;">                  
                    <a href="https://llava-vl.github.io/blog/2024-09-30-llava-video/" target="_blank" rel="noopener noreferrer">
                    <papertitle>LLaVA-Video: Video Instruction Tuning With Synthetic Data</papertitle>
                  </a>
                  <br />
                  <strong>Yuanhan Zhang</strong>,
                  <a href="https://scholar.google.com/citations?user=eh-XJIoAAAAJ&hl=zh-CN" target="_blank" rel="noopener noreferrer">Jinming Wu</a>,
                  <a href="https://scholar.google.com/citations?user=q8ZrKVIAAAAJ&hl=zh-CN" target="_blank" rel="noopener noreferrer">Wei Li</a>,
                  <a href="https://brianboli.com/" target="_blank" rel="noopener noreferrer">Bo Li</a>,
                  <a href="https://scholar.google.com/citations?user=XwY9LXoAAAAJ&hl=zh-CN" target="_blank" rel="noopener noreferrer">Zejun Ma</a>,
                  <a href="https://liuziwei7.github.io/" target="_blank" rel="noopener noreferrer">Ziwei Liu</a>,
                  <a href="https://chunyuan.li/" target="_blank" rel="noopener noreferrer">Chunyuan Li</a>
                  <br />
                  <em>TMLR</em>, 2025
                  <br />
                  <a href="https://arxiv.org/abs/2410.02713" target="_blank" rel="noopener noreferrer">PDF</a> /
                  <a href="https://llava-vl.github.io/blog/2024-09-30-llava-video/" target="_blank" rel="noopener noreferrer">Dataset, Model and Code</a>
                  <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/LLaVA-VL/LLaVA-NeXT?style=social" />
                  <p>Fully open-sourced video LMM (code, model, and data) with competitive ability.</p>
                </td>
              </tr>

              <tr>
                <td colspan="2" style="height:20px;"></td>
              </tr>

              <!-- LLaVA-OneVision -->
              <tr>
                  <td style="padding:5px 12px 5px 30px; width:300px; vertical-align:top;">                  
                    <div class="one">
                    <img src="images/llava-onevision.png"  width="260"  alt="LLaVA-OneVision teaser" loading="lazy" />
                  </div>
                </td>
                  <td style="padding:5px 0 5px 0; vertical-align:top;">                  
                    <a href="https://llava-vl.github.io/blog/2024-08-05-llava-onevision/" target="_blank" rel="noopener noreferrer">
                    <papertitle>LLaVA-OneVision: Easy Visual Task Transfer</papertitle>
                  </a>
                  <br />
                  <a href="https://brianboli.com/" target="_blank" rel="noopener noreferrer">Bo Li</a>,
                  <strong>Yuanhan Zhang</strong>,
                  <a href="https://www.linkedin.com/in/dongguoset" target="_blank" rel="noopener noreferrer">Dong Guo</a>,
                  <a href="https://zrrskywalker.github.io/" target="_blank" rel="noopener noreferrer">Renrui Zhang</a>,
                  <a href="https://fengli-ust.github.io/" target="_blank" rel="noopener noreferrer">Feng Li</a>,
                  <a href="https://haozhang534.github.io/" target="_blank" rel="noopener noreferrer">Hao Zhang</a>,
                  <a href="https://www.linkedin.com/in/kaichen-zhang-014b17219/?originalSubdomain=sg" target="_blank" rel="noopener noreferrer">Kaichen Zhang</a>,
                  <a href="https://yanwei-li.com/" target="_blank" rel="noopener noreferrer">Yanwei Li</a>,
                  <a href="https://liuziwei7.github.io/" target="_blank" rel="noopener noreferrer">Ziwei Liu</a>,
                  <a href="https://chunyuan.li/" target="_blank" rel="noopener noreferrer">Chunyuan Li</a>
                  <br />
                  <em>TMLR</em>, 2025
                  <br />
                  <a href="https://arxiv.org/abs/2408.03326" target="_blank" rel="noopener noreferrer">PDF</a> /
                  <a href="https://llava-vl.github.io/blog/2024-08-05-llava-onevision/" target="_blank" rel="noopener noreferrer">Dataset and Code</a>
                  <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/LLaVA-VL/LLaVA-NeXT?style=social" />
                  <p>A family of LMMs consolidating insights into data, models, and visual representations.</p>
                </td>
              </tr>


              <tr>
                <td colspan="2" style="height:20px;"></td>
              </tr>

              <!-- LLaVA-NeXT-Interleave -->
              <tr>
                  <td style="padding:5px 12px 5px 30px; width:300px; vertical-align:top;">                  
                    <div class="one">
                    <img src="images/llava_interleaved.png"  width="260"  alt="LLaVA-NeXT-Interleave teaser" loading="lazy" />
                  </div>
                </td>
                  <td style="padding:5px 0 5px 0; vertical-align:top;">                  
                    <a href="https://llava-vl.github.io/blog/2024-06-16-llava-next-interleave/" target="_blank" rel="noopener noreferrer">
                    <papertitle>LLaVA-NeXT-Interleave: Tackling Multi-image, Video, and 3D in Large Multimodal Models</papertitle>
                  </a>
                  <br />
                  <a href="https://fengli-ust.github.io/" target="_blank" rel="noopener noreferrer">Feng Li<sup>*</sup></a>,
                  <a href="https://zrrskywalker.github.io/" target="_blank" rel="noopener noreferrer">Renrui Zhang<sup>*</sup></a>,
                  <a href="https://scholar.google.com/citations?user=B8hPxMQAAAAJ&hl=en" target="_blank" rel="noopener noreferrer">Hao Zhang<sup>*</sup></a>,
                  <strong>Yuanhan Zhang</strong>,
                  <a href="https://brianboli.com/" target="_blank" rel="noopener noreferrer">Bo Li</a>,
                  <a href="https://scholar.google.com/citations?user=q8ZrKVIAAAAJ&hl=zh-CN" target="_blank" rel="noopener noreferrer">Wei Li</a>,
                  <a href="https://scholar.google.com/citations?user=XwY9LXoAAAAJ&hl=zh-CN" target="_blank" rel="noopener noreferrer">Zejun Ma</a>
                  <br />
                  <em>ICLR</em>, 2025 (Spotlight)
                  <br />
                  <a href="https://arxiv.org/pdf/2407.07895" target="_blank" rel="noopener noreferrer">PDF</a> /
                  <a href="https://llava-vl.github.io/blog/2024-06-16-llava-next-interleave/" target="_blank" rel="noopener noreferrer">Dataset and Code</a>
                  <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/LLaVA-VL/LLaVA-NeXT?style=social" />
                  <p>Tackling multi-image, video, and 3D in large multimodal models.</p>
                </td>
              </tr>


              <tr>
                <td colspan="2" style="height:20px;"></td>
              </tr>

              <!-- MMBench -->
              <tr>
                  <td style="padding:5px 12px 5px 30px; width:300px; vertical-align:top;">                  
                    <div class="one">
                    <img src="images/mmbench.png"  width="260"  alt="MMBench logo" loading="lazy" />
                  </div>
                </td>
                  <td style="padding:5px 0 5px 0; vertical-align:top;">                  
                    <a href="https://mmbench.opencompass.org.cn/home" target="_blank" rel="noopener noreferrer">
                    <papertitle>MMBench: Is Your Multi-modal Model an All-around Player?</papertitle>
                  </a>
                  <br />
                  <a href="https://yuanliuuuuuu.github.io/" target="_blank" rel="noopener noreferrer">Yuan Liu<sup>*</sup></a>,
                  <a href="https://kennymckormick.github.io/" target="_blank" rel="noopener noreferrer">Haodong Duan<sup>*</sup></a>,
                  <strong>Yuanhan Zhang<sup>*</sup></strong>,
                  <a href="https://brianboli.com/" target="_blank" rel="noopener noreferrer">Bo Li<sup>*</sup></a>,
                  <a href="https://tonysy.github.io/" target="_blank" rel="noopener noreferrer">Songyang Zhang<sup>*</sup></a>,
                  <a href="https://scholar.google.com.hk/citations?user=aocj89kAAAAJ&hl=zh-CN" target="_blank" rel="noopener noreferrer">Wangbo Zhao</a>,
                  <a href="https://github.com/yyk-wew" target="_blank" rel="noopener noreferrer">Yike Yuan</a>,
                  <a href="https://myownskyw7.github.io/" target="_blank" rel="noopener noreferrer">Jiaqi Wang</a>,
                  <a href="https://conghui.github.io/" target="_blank" rel="noopener noreferrer">Conghui He</a>,
                  <a href="https://liuziwei7.github.io/" target="_blank" rel="noopener noreferrer">Ziwei Liu</a>,
                  <a href="https://chenkai.site/" target="_blank" rel="noopener noreferrer">Kai Chen</a>,
                  <a href="http://dahua.site/" target="_blank" rel="noopener noreferrer">Dahua Lin</a>
                  <br />
                  <em>ECCV</em>, 2024 (Oral)
                  <br />
                  <a href="https://arxiv.org/abs/2307.06281" target="_blank" rel="noopener noreferrer">PDF</a> /
                  <a href="https://mmbench.opencompass.org.cn/home" target="_blank" rel="noopener noreferrer">Dataset and Code</a>
                  <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/open-compass/MMBench?style=social" />
                  <p>Benchmarking 20 abilities of vision-language models.</p>
                </td>
              </tr>

              <tr>
                <td colspan="2" style="height:20px;"></td>
              </tr>

              <!-- Octopus -->
              <tr>
                  <td style="padding:5px 12px 5px 30px; width:300px; vertical-align:top;">                  
                    <div class="one">
                    <img src="images/octopus_logo.jpg"  width="260"  alt="Octopus logo" loading="lazy" />
                  </div>
                </td>
                  <td style="padding:5px 0 5px 0; vertical-align:top;">                  
                    <a href="https://choiszt.github.io/Octopus/" target="_blank" rel="noopener noreferrer">
                    <papertitle>Octopus: Embodied Vision-Language Programmer from Environmental Feedback</papertitle>
                  </a>
                  <br />
                  <a href="https://jingkang50.github.io/" target="_blank" rel="noopener noreferrer">Jingkang Yang</a>,
                  <a href="https://github.com/dongyh20" target="_blank" rel="noopener noreferrer">Yuhan Dong</a>,
                  <a href="https://github.com/choiszt" target="_blank" rel="noopener noreferrer">Shuai Liu</a>,
                  <a href="https://brianboli.com/" target="_blank" rel="noopener noreferrer">Bo Li</a>,
                  Ziyue Wang, Chencheng Jiang, Haoran Tan, Jiamu Kang,
                  <strong>Yuanhan Zhang</strong>,
                  <a href="https://kaiyangzhou.github.io/" target="_blank" rel="noopener noreferrer">Kaiyang Zhou</a>,
                  <a href="https://liuziwei7.github.io/" target="_blank" rel="noopener noreferrer">Ziwei Liu</a>
                  <br />
                  <em>ECCV</em>, 2024
                  <br />
                  <a href="https://arxiv.org/abs/2310.08588" target="_blank" rel="noopener noreferrer">PDF</a> /
                  <a href="https://choiszt.github.io/Octopus/" target="_blank" rel="noopener noreferrer">Dataset and Code</a>
                  <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/dongyh20/Octopus?style=social" />
                  <p>An embodied VLM trained with RLEF, strong at embodied visual planning and programming.</p>
                </td>
              </tr>

              <tr>
                <td colspan="2" style="height:20px;"></td>
              </tr>

              <!-- FunQA -->
              <tr>
                  <td style="padding:5px 12px 5px 30px; width:300px; vertical-align:top;">                  
                    <div class="one">
                    <img src="images/funqa_logo.jpg"  width="260"  alt="FunQA logo" loading="lazy" />
                  </div>
                </td>
                  <td style="padding:5px 0 5px 0; vertical-align:top;">                  
                    <a href="https://funqa-benchmark.github.io/" target="_blank" rel="noopener noreferrer">
                    <papertitle>FunQA: Towards Surprising Video Comprehension</papertitle>
                  </a>
                  <br />
                  <a href="https://github.com/Nicous20" target="_blank" rel="noopener noreferrer">Binzhu Xie</a>,
                  <a href="https://github.com/fesvhtr" target="_blank" rel="noopener noreferrer">Sicheng Zhang</a>,
                  <a href="https://github.com/Zzitang" target="_blank" rel="noopener noreferrer">Zitang Zhou</a>,
                  <a href="https://brianboli.com/" target="_blank" rel="noopener noreferrer">Bo Li</a>,
                  <strong>Yuanhan Zhang</strong>,
                  <a href="https://jmhessel.com/" target="_blank" rel="noopener noreferrer">Jack Hessel</a>,
                  <a href="https://jingkang50.github.io/" target="_blank" rel="noopener noreferrer">Jingkang Yang</a>,
                  <a href="https://liuziwei7.github.io/" target="_blank" rel="noopener noreferrer">Ziwei Liu</a>
                  <br />
                  <em>ECCV</em>, 2024
                  <br />
                  <a href="https://arxiv.org/abs/2306.14899" target="_blank" rel="noopener noreferrer">PDF</a> /
                  <a href="https://funqa-benchmark.github.io/" target="_blank" rel="noopener noreferrer">Dataset and Code</a>
                  <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/Nicous20/FunQA?style=social" />
                  <p>Benchmarks funny, creative, and magic videos for challenging video understanding.</p>
                </td>
              </tr>

              <tr>
                <td colspan="2" style="height:20px;"></td>
              </tr>
              <!-- Otter -->
              <tr>
                  <td style="padding:5px 12px 5px 30px; width:300px; vertical-align:top;">                  
                    <div class="one">
                    <img src="https://i.postimg.cc/mksBCbV9/brand-title.png"  width="260"  alt="Otter brand title" loading="lazy" />
                  </div>
                </td>
                  <td style="padding:5px 0 5px 0; vertical-align:top;">                  
                    <a href="https://github.com/Luodian/Otter" target="_blank" rel="noopener noreferrer">
                    <papertitle>Otter: A Multi-modal Model with In-context Instruction Tuning</papertitle>
                  </a>
                  <br />
                  <a href="https://brianboli.com/" target="_blank" rel="noopener noreferrer">Bo Li<sup>*</sup></a>,
                  <strong>Yuanhan Zhang<sup>*</sup></strong>,
                  <a href="https://cliangyu.com/" target="_blank" rel="noopener noreferrer">Liangyu Chen</a>,
                  <a href="https://king159.github.io/" target="_blank" rel="noopener noreferrer">Jinghao Wan</a>,
                  <a href="https://pufanyi.github.io/" target="_blank" rel="noopener noreferrer">Fanyi Pu</a>,
                  <a href="https://jingkang50.github.io/" target="_blank" rel="noopener noreferrer">Jingkang Yang</a>,
                  <a href="https://chunyuan.li/" target="_blank" rel="noopener noreferrer">Chunyuan Li</a>,
                  <a href="https://liuziwei7.github.io/" target="_blank" rel="noopener noreferrer">Ziwei Liu</a>
                  <br />
                  <em>TPAMI</em>
                  <br />
                  <a href="https://arxiv.org/abs/2306.05425" target="_blank" rel="noopener noreferrer">PDF</a> /
                  <a href="https://otter-ntu.github.io/" target="_blank" rel="noopener noreferrer">Dataset and Code</a>
                  <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/Luodian/Otter?style=social" />
                  <p>A vision-language model with in-context instruction tuning.</p>
                </td>
              </tr>

              <tr>
                <td colspan="2" style="height:20px;"></td>
              </tr>

              <!-- Knowledge-augmented animal recognition -->
              <!-- <tr>
                  <td style="padding:5px 12px 5px 0px; width:300px; vertical-align:top;">                  
                    <div class="one">
                    <img src="images/llm_animal.png"  width="260"  alt="Knowledge-augmented instruction tuning teaser" loading="lazy" />
                  </div>
                </td>
                  <td style="padding:5px 0 5px 0; vertical-align:top;">                  
                    <a href="https://zhangyuanhan-ai.github.io/" target="_blank" rel="noopener noreferrer">
                    <papertitle>Knowledge-augmented Instruction Tuning for Zero-shot Animal Species Recognition</papertitle>
                  </a>
                  <br />
                  <a href="https://z-fabian.github.io/" target="_blank" rel="noopener noreferrer">Zalan Fabian</a>,
                  <a href="https://scholar.google.com/citations?user=at4m2mYAAAAJ&hl=en" target="_blank" rel="noopener noreferrer">Zhongqi Miao</a>,
                  <a href="https://chunyuan.li/" target="_blank" rel="noopener noreferrer">Chunyuan Li</a>,
                  <strong>Yuanhan Zhang</strong>,
                  <a href="https://liuziwei7.github.io/" target="_blank" rel="noopener noreferrer">Ziwei Liu</a>,
                  <a href="https://cider.uniandes.edu.co/en/professors/andres-hernandez-qui%C3%B1onez" target="_blank" rel="noopener noreferrer">Andr√©s Hern√°ndez</a>,
                  <a href="https://scholar.google.com/citations?user=J7eAM2sAAAAJ&hl=es" target="_blank" rel="noopener noreferrer">Andr√©s Montes-Rojas</a>,
                  <a href="https://www.linkedin.com/in/rafael-santiago-escucha-ram%C3%ADrez-a4149a227/?originalSubdomain=co" target="_blank" rel="noopener noreferrer">Rafael Escucha</a>,
                  <a href="https://orcid.org/0009-0005-9422-130X" target="_blank" rel="noopener noreferrer">Laura Siabatto</a>,
                  <a href="https://scholar.google.es/citations?user=BkuODsEAAAAJ&hl=es" target="_blank" rel="noopener noreferrer">Andr√©s Link</a>,
                  <a href="https://biomedicalcomputervision.uniandes.edu.co/" target="_blank" rel="noopener noreferrer">Pablo Arbel√°ez</a>,
                  <a href="https://www.microsoft.com/en-us/research/people/radodhia/" target="_blank" rel="noopener noreferrer">Rahul Dodhia</a>,
                  <a href="https://www.microsoft.com/en-us/research/people/jlavista/" target="_blank" rel="noopener noreferrer">Juan Lavista Ferres</a>
                  <br />
                  <em>Instruction Tuning and Instruction Following Workshop @ NeurIPS 2023</em>
                  <br /><br />
                  <a href="https://arxiv.org/abs/2311.01064" target="_blank" rel="noopener noreferrer">PDF</a>
                  <p>A knowledge-augmented VLM for AI conservation.</p>
                </td>
              </tr> -->

              <tr>
                <td colspan="2" style="height:20px;"></td>
              </tr>
              <!-- Visual Prompt Retrieval -->
              <tr>
                  <td style="padding:5px 12px 5px 30px; width:300px; vertical-align:top;">                  
                    <div class="one">
                    <img src="images/visual_prompt_retrieval.png"  width="260"  alt="Visual Prompt Retrieval teaser" loading="lazy" />
                  </div>
                </td>
                  <td style="padding:5px 0 5px 0; vertical-align:top;">                  
                    <a href="https://github.com/ZhangYuanhan-AI/visual_prompt_retrieval" target="_blank" rel="noopener noreferrer">
                    <papertitle>What Makes Good Examples for Visual In-Context Learning?</papertitle>
                  </a>
                  <br />
                  <strong>Yuanhan Zhang</strong>,
                  <a href="https://kaiyangzhou.github.io/" target="_blank" rel="noopener noreferrer">Kaiyang Zhou</a>,
                  <a href="https://liuziwei7.github.io/" target="_blank" rel="noopener noreferrer">Ziwei Liu</a>
                  <br />
                  <em>NeurIPS</em>, 2023
                  <br />
                  <a href="https://arxiv.org/abs/2301.13670" target="_blank" rel="noopener noreferrer">PDF</a> /
                  <a href="https://github.com/ZhangYuanhan-AI/visual_prompt_retrieval" target="_blank" rel="noopener noreferrer">Code</a>
                  <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/ZhangYuanhan-AI/visual_prompt_retrieval?style=social" />
                  <p>Retrieving prompts for visual in-context learning.</p>
                </td>
              </tr>
              
              <tr>
                <td colspan="2" style="height:20px;"></td>
              </tr>
              <!-- PROOF -->
              <tr>
                  <td style="padding:5px 12px 5px 30px; width:300px; vertical-align:top;">                  
                    <div class="one">
                    <img src="images/CIL.png"  width="260"  alt="Learning without Forgetting teaser" loading="lazy" />
                  </div>
                </td>
                  <td style="padding:5px 0 5px 0; vertical-align:top;">                  
                    <a href="https://github.com/zhoudw-zdw/PROOF" target="_blank" rel="noopener noreferrer">
                    <papertitle>Learning without Forgetting for Vision-Language Models</papertitle>
                  </a>
                  <br />
                  <a href="http://www.lamda.nju.edu.cn/zhoudw" target="_blank" rel="noopener noreferrer">Da-Wei Zhou</a>,
                  <strong>Yuanhan Zhang</strong>,
                  <a href="#" target="_blank" rel="noopener noreferrer">Yan Wang</a>,
                  <a href="https://jingyinju.github.io/" target="_blank" rel="noopener noreferrer">Jingyi Ning</a>,
                  <a href="http://www.lamda.nju.edu.cn/yehj" target="_blank" rel="noopener noreferrer">Han-Jia Ye</a>,
                  <a href="http://www.lamda.nju.edu.cn/zhandc" target="_blank" rel="noopener noreferrer">De-Chuan Zhan</a>,
                  <a href="http://liuziwei7.github.io/" target="_blank" rel="noopener noreferrer">Ziwei Liu</a>
                  <br />
                  <em>TPAMI</em>
                  <br />
                  <a href="https://arxiv.org/abs/2305.19270v1" target="_blank" rel="noopener noreferrer">PDF</a> /
                  <a href="https://github.com/zhoudw-zdw/PROOF" target="_blank" rel="noopener noreferrer">Code</a>
                  <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/zhoudw-zdw/PROOF?style=social" />
                  <p>Learning without forgetting for vision-language models.</p>
                </td>
              </tr>

              <tr>
                <td colspan="2" style="height:20px;"></td>
              </tr>

              <!-- NOAH -->
              <!-- <tr onmouseover="noah_start()" onmouseout="noah_stop()"> -->
                  <!-- <td style="padding:5px 12px 5px 30px; width:300px; vertical-align:top;">                  <div id="noah_anim" style="display:none;">
                    <img src="images/NOAH_annimation.gif"  width="260"  height='300' ="NOAH animation" style="border-style:none;" loading="lazy" />
                  </div> -->
                  <!-- <div id="noah_still" style="display:inline;"> -->
                    <!-- <img src="images/NOAH_teaser.png"  width="260"  alt="NOAH teaser" style="border-style:none;" loading="lazy" /> -->
                  <!-- </div> -->
                  <!-- <script>
                    function noah_start() {
                      document.getElementById('noah_anim').style.display = 'inline';
                      document.getElementById('noah_still').style.display = 'none';
                    }
                    function noah_stop() {
                      document.getElementById('noah_anim').style.display = 'none';
                      document.getElementById('noah_still').style.display = 'inline';
                    }
                    noah_stop();
                  </script> -->
                
                <td style="padding:5px 12px 5px 30px; width:300px; vertical-align:top;">                  
                  <div class="one">
                    <img src="images/NOAH_teaser.png"  width="260"  alt="NOAH teaser" style="border-style:none;" loading="lazy" />
                  </div>
                </td>
                  <td style="padding:5px 0 5px 0; vertical-align:top;">  
                    <a href="https://github.com/Davidzhangyuanhan/NOAH" target="_blank" rel="noopener noreferrer">
                    <papertitle>Neural Prompt Search</papertitle>
                  </a>
                  <br />
                  <strong>Yuanhan Zhang</strong>,
                  <a href="https://kaiyangzhou.github.io/" target="_blank" rel="noopener noreferrer">Kaiyang Zhou</a>,
                  <a href="https://liuziwei7.github.io/" target="_blank" rel="noopener noreferrer">Ziwei Liu</a>
                  <br />
                  <em>TPAMI</em>
                  <br />
                  <a href="https://arxiv.org/abs/2206.04673" target="_blank" rel="noopener noreferrer">PDF</a> /
                  <a href="https://zhangyuanhan-ai.github.io/NOAH" target="_blank" rel="noopener noreferrer">Project Page</a> /
                  <a href="https://github.com/Davidzhangyuanhan/NOAH" target="_blank" rel="noopener noreferrer">Code</a>
                  <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/Davidzhangyuanhan/NOAH?style=social" />
                  <p>Searching prompt modules for parameter-efficient transfer learning.</p>
                </td>
              </tr>

              <tr>
                <td colspan="2" style="height:20px;"></td>
              </tr>

              <!-- 3D KD -->
              <tr>
                  <td style="padding:5px 12px 5px 30px; width:300px; vertical-align:top;">                  
                    <div class="one">
                    <img src="images/point_cloud_KD.png"  width="260"  alt="3D point cloud KD teaser" loading="lazy" />
                  </div>
                </td>
                  <td style="padding:5px 0 5px 0; vertical-align:top;">                  
                    <a href="https://arxiv.org/abs/2212.08974" target="_blank" rel="noopener noreferrer">
                    <papertitle>3D Point Cloud Pre-training with Knowledge Distillation from 2D Images?</papertitle>
                  </a>
                  <br />
                  <a href="https://www.cs.rochester.edu/u/yyao39/#intro" target="_blank" rel="noopener noreferrer">Yuan Yao</a>,
                  <strong>Yuanhan Zhang</strong>,
                  <a href="https://yinzhenfei.github.io/" target="_blank" rel="noopener noreferrer">Zhenfei Yin</a>,
                  <a href="https://cs.rochester.edu/u/jluo/" target="_blank" rel="noopener noreferrer">Jiebo Luo</a>,
                  <a href="https://wlouyang.github.io/" target="_blank" rel="noopener noreferrer">Wanli Ouyang</a>,
                  <a href="https://xiaoshuihuang.github.io/" target="_blank" rel="noopener noreferrer">Xiaoshui Huang</a>
                  <br />
                  <em>ICME</em>, 2023
                  <br />
                  <a href="https://arxiv.org/abs/2212.08974" target="_blank" rel="noopener noreferrer">PDF</a> /
                  <a href="https://arxiv.org/abs/2212.08974" target="_blank" rel="noopener noreferrer">Code</a>
                  <p>3D point cloud pre-training with knowledge distillation from 2D images.</p>
                </td>
              </tr>

              <tr>
                <td colspan="2" style="height:20px;"></td>
              </tr>

              <!-- OmniBenchmark -->
              <tr>
                  <td style="padding:5px 12px 5px 30px; width:300px; vertical-align:top;">                  
                    <div class="one">
                    <img src="images/OmniBenchmark.png"  width="260"  alt="OmniBenchmark teaser" loading="lazy" />
                  </div>
                </td>
                  <td style="padding:5px 0 5px 0; vertical-align:top;">                  
                    <a href="https://zhangyuanhan-ai.github.io/OmniBenchmark" target="_blank" rel="noopener noreferrer">
                    <papertitle>Benchmarking Omni-Vision Representation through the Lens of Visual Realms</papertitle>
                  </a>
                  <br />
                  <strong>Yuanhan Zhang</strong>,
                  <a href="https://yinzhenfei.github.io/" target="_blank" rel="noopener noreferrer">Zhenfei Yin</a>,
                  <a href="https://amandajshao.github.io/" target="_blank" rel="noopener noreferrer">Jing Shao</a>,
                  <a href="https://liuziwei7.github.io/" target="_blank" rel="noopener noreferrer">Ziwei Liu</a>
                  <br />
                  <em>ECCV</em>, 2022
                  <br />
                  <a href="https://arxiv.org/abs/2207.07106" target="_blank" rel="noopener noreferrer">PDF</a> /
                  <a href="https://zhangyuanhan-ai.github.io/OmniBenchmark" target="_blank" rel="noopener noreferrer">Project Page</a> /
                  <a href="https://paperswithcode.com/sota/image-classification-on-omnibenchmark" target="_blank" rel="noopener noreferrer">Leaderboard</a> /
                  <a href="https://codalab.lisn.upsaclay.fr/competitions/6043" target="_blank" rel="noopener noreferrer">Challenge: ImageNet-1k Pretrain Track</a> /
                  <a href="https://codalab.lisn.upsaclay.fr/competitions/6045" target="_blank" rel="noopener noreferrer">Challenge: Open-Pretrain Track</a> /
                  <a href="https://github.com/ZhangYuanhan-AI/OmniBenchmark" target="_blank" rel="noopener noreferrer">Dataset and Code</a>
                  <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/ZhangYuanhan-AI/OmniBenchmark?style=social" />
                  <p>New benchmark for evaluating vision foundation models; supervised contrastive learning framework.</p>
                </td>
              </tr>

              <tr>
                <td colspan="2" style="height:20px;"></td>
              </tr>

              <!-- CelebA-Spoof -->
              <tr>
                  <td style="padding:5px 12px 5px 30px; width:300px; vertical-align:top;">                  
                    <div class="one">
                    <img src="images/spoofchallenge_logo.png"  width="260"  alt="CelebA-Spoof logo" loading="lazy" />
                  </div>
                </td>
                  <td style="padding:5px 0 5px 0; vertical-align:top;">                  
                    <a href="https://github.com/Davidzhangyuanhan/CelebA-Spoof" target="_blank" rel="noopener noreferrer">
                    <papertitle>CelebA-Spoof: Large-Scale Face Anti-Spoofing Dataset with Rich Annotations</papertitle>
                  </a>
                  <br />
                  <strong>Yuanhan Zhang</strong>,
                  <a href="https://yinzhenfei.github.io/" target="_blank" rel="noopener noreferrer">Zhenfei Yin</a>,
                  <a href="https://faculty.bjtu.edu.cn/8408/" target="_blank" rel="noopener noreferrer">Yidong Li</a>,
                  <a href="https://gjyin91.github.io/" target="_blank" rel="noopener noreferrer">Guojun Yin</a>,
                  <a href="https://yan-junjie.github.io/" target="_blank" rel="noopener noreferrer">Junjie Yan</a>,
                  <a href="https://amandajshao.github.io/" target="_blank" rel="noopener noreferrer">Jing Shao</a>,
                  <a href="https://liuziwei7.github.io/" target="_blank" rel="noopener noreferrer">Ziwei Liu</a>
                  <br />
                  <em>ECCV</em>, 2020
                  <br />
                  <a href="https://arxiv.org/abs/2007.12342" target="_blank" rel="noopener noreferrer">PDF</a> /
                  <a href="https://mmlab.ie.cuhk.edu.hk/projects/CelebA/CelebA_Spoof.html" target="_blank" rel="noopener noreferrer">Dataset</a> /
                  <a href="https://www.youtube.com/watch?v=A7XjSg5srvI" target="_blank" rel="noopener noreferrer">Demo</a> /
                  <a href="https://github.com/Davidzhangyuanhan/CelebA-Spoof" target="_blank" rel="noopener noreferrer">Code</a>
                  <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/Davidzhangyuanhan/CelebA-Spoof?style=social" />
                  <p>Large-scale face anti-spoofing dataset.</p>
                </td>
              </tr>


            </tbody>
          </table>

          <!-- Activities -->
          <table style="width:100%;border:0;border-spacing:0;border-collapse:separate;margin:0 auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle;">
                  <heading role="heading" aria-level="2">Activities</heading>
                </td>
              </tr>
            </tbody>
          </table>
          <ul>
            <li>Reviewer: CVPR / ECCV / ICCV / ICLR / NeurIPS / TPAMI / IJCV</li>
            <li>
              Organizing Committee:
              <a href="https://theaitalks.org/" target="_blank" rel="noopener noreferrer">The AI Talk</a>,
              CVPR 2024 Workshop (<a href="https://prompting-in-vision.github.io/" target="_blank" rel="noopener noreferrer">Prompting in Vision</a>),
              ECCV 2022 Workshops
              (<a href="https://sense-human.github.io/" target="_blank" rel="noopener noreferrer">workshop1</a>,
              <a href="https://computer-vision-in-the-wild.github.io/eccv-2022/" target="_blank" rel="noopener noreferrer">workshop2</a>)
            </li>
          </ul>

          <!-- Public Office Hour -->
          <table style="width:100%;border:0;border-spacing:0;border-collapse:separate;margin:0 auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle;">
                  <heading role="heading" aria-level="2">Public Office Hour</heading>
                </td>
              </tr>
            </tbody>
          </table>
          <ul>
            <li>
              <!-- Calendly link widget begin -->
              <link href="https://assets.calendly.com/assets/external/widget.css" rel="stylesheet" />
              <script src="https://assets.calendly.com/assets/external/widget.js" type="text/javascript" async></script>
              <a href="#" aria-label="Book a 15-minute slot on Calendly"
                 onclick="Calendly.initPopupWidget({url: 'https://calendly.com/zhangyuanhan/15min'});return false;">
                 Happy to chat about any topics :)
              </a>
              <!-- Calendly link widget end -->
            </li>
          </ul>

          <!-- Footer -->
          <table style="width:100%;border:0;border-spacing:0;border-collapse:separate;margin:0 auto;">
            <tbody>
              <tr>
                <td style="padding:0;">
                  <br />
                  <p style="text-align:right;font-size:small;">Last updated in Aug. 2025.</p>
                  <p style="text-align:right;font-size:small;">Homepage credits:
                    <a href="https://github.com/jonbarron/jonbarron_website" target="_blank" rel="noopener noreferrer">Jon Barron</a>.
                  </p>
                </td>
              </tr>
            </tbody>
          </table>

        </td>
      </tr>
    </tbody>
  </table>
</body>
</html>
